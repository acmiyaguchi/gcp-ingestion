#!/usr/bin/env python3

"""Download samples of json documents from the decoded and error stream."""

import argparse
import logging
import json
import os

from google.cloud import bigquery

INGESTION_BEAM_ROOT = os.path.realpath(
    os.path.join(os.path.dirname(os.path.realpath(__file__)), "..")
)

# formatted using the BigQuery console formatter
QUERY = """
-- Create a PubSub compatible row with the most recent document samples that
-- have been decoded.
SELECT
  STRUCT( document_namespace,
    document_type,
    document_version ) AS attributeMap,
  payload
FROM
  `moz-fx-data-shared-prod`.monitoring.document_sample_nonprod_v1
WHERE
  document_decoded
  AND submission_timestamp = (
  SELECT
    MAX(submission_timestamp)
  FROM
    monitoring.document_sample_nonprod_v1 )
ORDER BY
  document_namespace,
  document_type,
  document_version
  LIMIT 100
"""


def extract_samples():
    """A generator for reading documents in the sampled landfill dataset that exist in the schemas."""
    client = bigquery.Client()
    results = client.query(QUERY)
    for row in query_job:
        print(row)
    

def main(args):
    os.chdir(INGESTION_BEAM_ROOT)
    documents = extract_samples()
    logging.info("Done!")


def parse_arguments():
    parser = argparse.ArgumentParser("download-sampled-landfill")
    parser.add_argument("--bucket", default="telemetry-parquet")
    parser.add_argument("--prefix", default="sanitized-landfill-sample/v3")
    parser.add_argument("--output-file", default="document_sample.ndjson")
    args = parser.parse_args()
    return args


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    main(parse_arguments())
